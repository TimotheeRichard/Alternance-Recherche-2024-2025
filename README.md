# I.2.b Analyse de la convergence des cha√Ænes de Markov

## 1. Code d'analyse de convergence des cha√Ænes

**Le code ci-dessous permet d'analyser si les cha√Ænes de Markov produites sont convergentes ou non selon diff√©rents facteurs. Un fichier .out r√©sumant ce que les diff√©rents crit√®res affirment sur la convergence sera alors produit. La convergence des cha√Ænes est n√©cessaire pour utiliser la m√©thode MCMC.**

```R
##########################libraries###################
library(coda)
library(KernSmooth)
library(ggplot2)
library(RColorBrewer)
library(latex2exp)
library(scales)
library(lubridate)

####################################################

getwd()
setwd("C:/Users/richa/OneDrive/Desktop/Alternance recherche/Bayesien Informatique/Deal_MCMC/TRAJECTORY")
rm(list = ls())
source("../R/diagnoseChains.R") #apporte plein de fonction que l'on va utiliser dans la suite du code

### Traitement d'un bassin versant
List_Catchment = c("Meuse","Loire","Moselle", "Rhin", "Rhone")

Name_catchment = List_Catchment[4]
Rep_MCMC = "../../mcsim_under_R/MCSim_under_R/Trajectory/"

file1 = paste(Rep_MCMC,Name_catchment,"/Out/Chain1/trajectory.MCMC_supp",1,".out",sep="")
file2 = paste(Rep_MCMC,Name_catchment,"/Out/Chain2/trajectory.MCMC_supp",2,".out",sep="")
file3 = paste(Rep_MCMC,Name_catchment,"/Out/Chain3/trajectory.MCMC_supp",3,".out",sep="")

### Recuperation des 3 chaines supp

outchain1 = read.table(file = file1, sep ="\t", header = TRUE) # Premi√®re cha√Æne de  Markov √† 4800 it√©rations
outchain2 = read.table(file = file2, sep ="\t", header = TRUE) # Deuxi√®me cha√Æne de  Markov √† 4800 it√©rations
outchain3 = read.table(file = file3, sep ="\t", header = TRUE) # Troisi√®me cha√Æne de  Markov √† 4800 it√©rations
head(outchain1)
NbChains = 3 # Nombre de CM
res = list() # Initialisation avec une liste vide
chain=c() # Initialisation avec une liste de cha√Ænes de caract√®res vide
#i=1
for(i in 1:NbChains)
{
  chain <- append(chain, paste('chain',i,sep='')) # ["chain1","chain2","chain3"]
  res[[i]]=list(paste("LnPosterior.chain",i,sep=""),paste("output.chain",i,sep='')) #Titre des deux listes de res[[i]]
  
  data = read.table(file = paste(Rep_MCMC,Name_catchment,"/Out/Chain",i,"/trajectory.MCMC_supp",i,".out",sep=""), sep ="\t", header = TRUE) # R√©cup√®re la i-√®me cha√Æne de Markov
  res[[i]][[1]] = data[,"LnPosterior"] #S√©lectionne la colonne des LnPosterior de Data et le met dans la premi√®re liste de res[[i]]
  data = as.data.frame(data[,c(-dim(data)[2],-dim(data)[2]+1,-dim(data)[2]+2)]) # Suppression des trois derni√®res colonnes de data dans data (LnPrior, LnData, LnPosterior)
  res[[i]][[2]] = as.data.frame(data[,c(-1)]) # Met toutes les colonnes de data dans res[[i]][[2]] (except√© la premi√®re) (donc Tmig, pero et sigmasq_AM)
  num = length(names(data)[2:dim(data)[2]]) # Met dans num le nbr de colonnes de data, √† l'exception de la premi√®re colonne (ce sera 3 ici du coup)
  Nbiteration = dim(paste('output.chain',i,sep=''))[1] # √ßa donne Nbiteration = NULL donc jsp √† quoi √ßa sert
  for(j in 1:num)
  {
    names(res[[i]][[2]])[j] = substr(names(data)[j+1],1, nchar(names(data)[j+1])-3) # Donne le nom de Tmig, etc aux listes de output.chaini
  }
  names(res[[i]]) <- c(paste("LnPosterior.chain",i,sep=""), paste("output.chain",i,sep='')) # √ßa redonne les noms au deux sous listes mais jsp pourquoi
}
names(res) <- chain # Nomme les listes de res (Chain1, Chain2, Chain3)

parNamesVal = NULL
for(i in 1:num)
{
  parNamesVal <- c(parNamesVal,list(substr(names(data)[i+1],1, nchar(names(data)[i+1])-3)))
}
# On a donc parNamesVal = ["Tmig", "pero",sigmasq_AM]
output.list<-list() # Initialisation d'une liste vide
for(i in res)
{
  output.list[[length(output.list)+1]] <- mcmc(i[[2]]) # mcmc permet de le rendre compatible avec coda
}
output.list <- mcmc.list(output.list) # On a output.list qui est la concat√©nation des trois cha√Æne de Markov et qui est sous une forme compatible avec la librairie coda

codachain<-list() # Initialisation d'une liste vide
codalnposterior<-list() # Pareil

for (i in 1:NbChains)
{
  codachain[[i]]=res[[i]][[2]] # codachain contient Tmig, pero et sigmasq_AM
  codalnposterior[[i]]=res[[i]][[1]] # codalnposterior contient lnposterior
}

num= ncol(codachain[[1]]) # La valeur de num change pas du coup, cette ligne ne sert √† rien je pense
list_read_coda = list(num=num, codachain=codachain,codalnposterior=codalnposterior,output.list=output.list) # Cr√©e une liste contenant les 4 √©l√©ments : num, codachain, codalnposterior, output.list
list_coda_convergence = deal_coda_convergence(list_read_coda[["codachain"]],list_read_coda[["codalnposterior"]],list_read_coda[["output.list"]], N_chain=NbChains)  

fileprefix=paste(Name_catchment,"/Imagessupp/Trajectory",sep="") # Cr√©e juste un chemin vers Imagesupp
cat("   preparing plots\n") #Envoir un message √† la console pour dire o√π on en est
plot_trails(fileprefix,list_read_coda[["num"]],list_coda_convergence[["sum"]], list_read_coda[["codachain"]],list_read_coda[["codalnposterior"]], N_chain=NbChains) # Permet d'obtenir Trajectory_trails.jpg dans Loire/Imagessupp/. On y trace les diff√©rente valeurs de Tmig, pero, sigmasq_AM et lnPosterior √† chaque it√©ration

densities = plot_density(fileprefix,list_read_coda[["num"]],list_coda_convergence[["sum"]], list_coda_convergence[["hpd"]],list_coda_convergence[["burnin.output.pooled"]]) #Trace les densit√© des trois param√®tres en faisant le fichier Trajectory_dens.jpg dans Imagessupp. Pour chaque param√®tre, il va estimer la densit√© avec la m√©thode de l'estimateur de noyau (bkde) avec une largeur de bande d√©termin√©e par dpik. Ensuite √ßa trace deux traits verticaux pour les quantiles 2,5% et 97,5% et aussi aux limites inf√©rieure et sup√©rieure de l'intervalle de haute densit√© post√©rieure

plot_lag(fileprefix,list_read_coda[["num"]],list_coda_convergence[["burnin.output.pooled"]]) # Fais le fichier Trajectory_lag.jpg qui trace pour chaque it√©ration n sa valeur sur l'axe des x et la valeur de l'it√©ration n+1 sur celle des y, si on observe que √ßa suit une droite lin√©aire c'est que √ßa converge s√ªrement pas, si √ßa commence √† former une biule c'est que y a convergence. On veut donc voir une sorte de boule se former

plot_autocorrelation(fileprefix,list_read_coda[["num"]],list_coda_convergence[["burnin.output.pooled"]]) # Cr√©e le fichier Trajectory_autocorr.jpg qui trace la corr√©lation d'une s√©rie avec elle-m√™me, pour analyser l'ind√©pendance des √©chantillons de la MCMC

plot_gelmanRubin(fileprefix,list_coda_convergence[["burnin.output.list"]]) # Cr√©e le graphique Trajectoryhat_gelmanRhat.jpg, √ßa trace le ration de r√©duction de variancee de Gelman-Rubin (Rhat) pour chaque param√®tre au fil des it√©rations. Un Rhat proche de 1 indique une bonne convergence

acceptrate  = acceptance_rate(list_coda_convergence[["burnin.output.pooled"]]) # Calcule le taux d'acceptation de chaque MCMC, √ßa correspond au pourcentage d'√©chantillons propos√©s qui sont accept√©s lors de la m√©thode de M-H
gelman      = test_gelman(list_read_coda[["num"]],list_coda_convergence[["burnin.output.list"]]) # Calcule le diagnostique de Gelman-Rubin, √ßa donne le psrf (Potential Scale Reduction Factor) (c'est le Rhat) et le mpsrf (Multivariate Potential Scale Reduction Factor) (un peu comme le psfr mais √ßa prend en compte la corr√©lation entre les diff√©rents param√®tres et donc si c'est proche de 1 √ßa indique que les cha√Ænes ont converg√©)
geweke      = test_geweke(list_read_coda[["num"]],list_coda_convergence[["burnin.output.list"]],list_coda_convergence[["chaintodisplay"]]) # Renseigne toi sur le diagnostic de Geweke qui renvoie un score Z (la diff√©rence entre les deux moyennes d‚Äô√©chantillons divis√©e par son erreur standard estim√©e) ( L‚Äôerreur standard est estim√©e √† partir de la densit√© spectrale √† z√©ro, ce qui prend en compte toute autocorr√©lation)
heidelberg  = test_heidelberg(list_read_coda[["num"]],list_coda_convergence[["burnin.output.list"]],list_coda_convergence[["chaintodisplay"]]) # Renseigne toi sur le test de Heiselberg
raftery     = list_coda_convergence[["raft"]]
parameters  = list_coda_convergence[["sum"]]

###
### sorties texte, ici on r√©sume tout ce qu'on vient d'obtenir dans un fichier texte
###
cat("   preparing text outputs\n")
nametxt=paste0(fileprefix, "_diagnoseChains.txt")
sink(nametxt)
cat("################################################################################################\n")
cat("\n\nRAFTERY\n\n")
print(raftery)
cat("################################################################################################\n")
cat("\n\nGEWEKE\n\n")
print(geweke)
cat("################################################################################################\n")
cat("\n\nHEIDELBERG\n\n")
print(heidelberg)
cat("################################################################################################\n")
cat("\n\nGELMAN\n\n")
print(gelman)
cat("################################################################################################\n")
cat("\n\nPARAMETERS\n\n")
print(parameters)
cat("################################################################################################\n")
sink()

###
### sortie RData
###
cat("   preparing .RData outputs\n")
nameRData=paste0(fileprefix, "_diagnoseChains.RData")
save(list=c( "parameters", "acceptrate", "densities", "raftery", "geweke", "heidelberg"), file=nameRData)
```

## 2. Crit√®re de Gelman-Rubin

**Sert √† √©valuer la convergence des MCMC, permet de d√©terminer si les chaines MCMC ont converg√© vers la distribution cible.**

On a plusieurs cha√Ænes MCMC avec des conditions initiales diff√©rentes. On va alors comparer les variances intra-cha√Æne et inter-cha√Æne. 

Si les cha√Ænes ont converg√© vers la m√™me distribution cible, les variances devraient √™tre similaires.
On a m cha√Ænes de longueur n.

**_Calculer la moyenne de chaque cha√Æne :_**

$$
\bar{\theta_i} = \frac{1}{n} \sum_{j=1}^{n} \theta_{i,j}
$$

**_Calculer la moyenne globale :_**

$$
\bar{\theta} = \frac{1}{m} \sum_{i=1}^{m} \bar{\theta_i}
$$

**_Calculer la variance intra-cha√Æne W :_**

$$
W = \frac{1}{m} \sum_{i=1}^{m} (\frac{1}{n-1} \sum_{j=1}^{n} (\theta_{i,j} - \bar{\theta_i})^2)
$$
 
**_Calculer la variance inter-cha√Æne B :_**

$$
B = \frac{n}{m-1} \sum_{i=1}^{m} (\bar{\theta_i} - \bar{\theta})^2
$$

**_Combiner les variances pour obtenir l'estimateur de la variance totale :_**

$$
\hat{V} = \frac{n-1}{n} W + \frac{1}{n} B
$$

**_Calculer le crit√®re de Gelman-Rubin :_**

$$
\hat{R} = \sqrt{\frac{\hat{V}}{W}}
$$
 
**_Interpr√©tation :_**

- **R‚âà1** : Indique que les cha√Ænes ont probablement converg√© vers la m√™me distribution cible. Un R proche de 1 (typiquement R<1.1) est souvent utilis√© comme crit√®re de convergence.

- **R>1** : Indique que les cha√Ænes n'ont pas encore converg√©. Cela signifie qu'il y a encore une diff√©rence notable entre les variances intra-cha√Æne et inter-cha√Æne, sugg√©rant que plus de simulations sont n√©cessaires.

## 3. Diagnostic de convergence de Geweke

**Ce diagnostic permet de v√©rifier la convergence des cha√Ænes MCMC, √ßa permet de v√©rifier si les √©chantillons provenant des diff√©rentes parties de la cha√Æne proviennent de la m√™me distribution stationnaire, ce qui indiquerait la convergence de la cha√Æne.**

√áa compare les moyennes de deux segments de la cha√Æne : le segment initial et le segment final. Si les deux segments proviennent de la m√™me distribution alors leurs moyennes devraient √™tre similaires et la diff√©rence normalis√©e devrait suivre une distribution normale standard.

**_Diviser la cha√Æne en segments :_**

Diviser la cha√Æne MCMC en deux segments non superpos√©s. Par exemple, en utilisant les 10 % premiers et les 50 % derniers √©chantillons de la cha√Æne.

**_Calculer les moyennes et les variances :_**

- Moyenne et variance du segment initial :

$$
\bar{X_A};\sigma_A^2
$$

- Moyenne et variance du segment final :

$$
\bar{X_B};\sigma_B^2
$$

**_Calculer la statistique de Geweke (Z-score) :_**

$$
Z = \frac{\bar{X_A} - \bar{X_B}}{\sqrt{\sigma_A^2 + \sigma_B^2}}
$$

**_Interpr√©tation :_**

- **Z-scores proches de z√©ro** : indiquent que les segments initiaux et finaux de la cha√Æne ont des moyennes similaires, sugg√©rant que la cha√Æne a converg√©.

- **Z-scores √©loign√©s de z√©ro** (au-del√† de ¬±1.96 ou d'autres seuils selon le niveau de confiance choisi) : indiquent une possible non-convergence de la cha√Æne, sugg√©rant que plus d'it√©rations ou un r√©ajustement du mod√®le pourraient √™tre n√©cessaires.

## 4. Test de stationarit√© de Heidelberger Welch

**Ce test examine la stationnarit√© de la cha√Æne et √ßa permet de d√©terminer si une partie initiale de la cha√Æne (le burn-in) doit √™tre supprim√©e pour obtenir des √©chantillons stationnaires.**

Ce crit√®re s'articule en deux tests : le test de stationnarit√© et celui de demi-largeur.

- **Test de stationnarit√©** : V√©rifie si la cha√Æne est stationnaire en examinant si les moyennes des sous-s√©quences de la cha√Æne sont stables.

- **Test de demi-largeur** : √âvalue la pr√©cision des estimations en v√©rifiant si la demi-largeur de l'intervalle de confiance pour la moyenne est suffisamment petite.


### Test de stationnarit√©

**_Diviser la cha√Æne en segments :_**

Diviser la cha√Æne en plusieurs segments (typiquement 10).

**_Calculer les moyennes des segments :_**

Calculer la moyenne de chaque segment.

**_Effectuer le test :_**

Utiliser un test statistique (comme un test de Student) pour comparer les moyennes des segments. Si les moyennes des segments ne sont pas significativement diff√©rentes, la cha√Æne est consid√©r√©e comme stationnaire.


### Test de demi-largeur

**_Calculer la moyenne et la variance de la cha√Æne :_**

Utiliser les √©chantillons restants apr√®s le burn-in pour en calculer la moyenne et la variance.

**_Calculer la demi-largeur de l'intervalle de confiance pour la moyenne :_**

$$
Demi largeur = z \frac{\sigma}{\sqrt{n}}
$$

o√π z est le quantile de la distribution normale standard correspondant au niveau de confiance choisi, ùúé est l'√©cart-type estim√© des √©chantillons, et ùëõ est le nombre d'√©chantillons.

**_Comparer avec la tol√©rance :_**

Si la demi-largeur est inf√©rieure √† un seuil de tol√©rance sp√©cifi√© (par exemple, 0.1 fois la moyenne), les √©chantillons sont consid√©r√©s comme suffisamment pr√©cis.
